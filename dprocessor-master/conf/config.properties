# data source
ds.driverClass=com.mysql.jdbc.Driver
ds.jdbcUrl=jdbc\:mysql\://192.168.88.2\:3306/DAE_TV2?useUnicode\=true&characterEncoding\=UTF-8
ds.username=root
ds.password=123456
# data source pool
ds.pool.minSize=6
ds.pool.maxSize=60
ds.pool.maxIdleTime=2000
ds.pool.maxStatements=50
ds.pool.preferredTestQuery=SELECT 1 FROM DUAL
ds.pool.idleConnectionTestPeriod=180
ds.pool.testConnectionOnCheckin=true
ds.pool.testConnectionOnCheckout=true

# hibernate
hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
hibernate.show_sql=false
hibernate.jdbc.fetch_size=30
hibernate.jdbc.batch_size=50
hibernate.cache.provider=org.hibernate.cache.EhCacheProvider
#baidu
baidu.proxyIP=
baidu.proxyPort=
#command
#cmd.featureExt.targetUrl="E:\\profiledata_06-May-2005\\imageRec\\100000.siftgeo.txt"
#cmd.featureExt.module="E:\\profiledata_06-May-2005\\imageRec\\clust_flickr60_k100.fvecs.txt"
#cmd.featureExt.output="out.txt"
cmd.command=/usr/local/usrJar/duliming/spark-1.5.1-bin-hadoop2.6/bin/
hive.hadoopFSDir=hdfs://192.168.88.10:8020/out/
hive.url=jdbc:hive2://192.168.88.12:21050/;auth=noSasl
hive.user=hdfs
hive.password=hdfs
hive.hiveDbName=default
hadoopFS.showDF=/out/
errorLog.dir=/root/dptest/dprocessor/out/
push.pushTimes=5
push.threshold=3000
jobserver.url=http\://192.168.88.11\:8090
jobserver.port=8090
jobServerOn.shell=/home/spark/job-server-master/server_start.sh
jobServerOff.shell=/home/spark/job-server-master/server_stop.sh
jobServer.pid=/home/spark/job-server-master/spark-jobserver.pid